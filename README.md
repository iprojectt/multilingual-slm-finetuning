[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-22041afd0340ce965d47ae6ef1cefeee28c7c493a6346c4f15d667ab976d596c.svg)](https://classroom.github.com/a/C8cH05zS)


# Report on Mini-Project: Phase 1 and Phase 2


- **Name**: Prakhar Raj
- **Roll No.**: 2022102066

## 1. Introduction
The goal of this mini-project is to build a small autoregressive language model (LM) for multiple languages:
English, Hindi & Awadhi from scratch. 

Given minimum total tokens required: 3 billion.
- 50% English - 1.5billion token
- 30-40% Hindi тЖТ 900 million - 1.2 billion token
- 10-20% Awadhi тЖТ 300-600 million

Also I will use Model - **Qwen3 (Dense)** 

 
 The project involves data collection, preprocessing, tokenizer training, and fine-tuning for specific tasks, mentioned as: [Go to Task 1](#task-1-ft72---text-simplification)
 & [Go to Task 2](#task-2-ft25---text-de-identification-anonymization)


##  Datasets

### Hugging Face Links

| Language     | Dataset Link |
|--------------|--------------|
| English      | [English Text Simplification](https://huggingface.co/datasets/raja20221020/english-text-simplification-for-finetuning) |
| Hindi        | [Hindi Text Simplification](https://huggingface.co/datasets/raja20221020/hindi-text-simplification-for-finetuning) |
| Awadhi       | [Awadhi Text Simplification](https://huggingface.co/datasets/raja20221020/awadhi-text-simplification-for-finetuning) |
| Eng_Hin_Awa  | [English-Hindi-Awadhi De-identification](https://huggingface.co/datasets/raja20221020/english_hindi_awadhi_deidentification) |

---

##  Experiment Tracking (Weights & Biases)

- **[Pretrained Model Logs](https://wandb.ai/prakhar_raj-iiit-hyderabad/lma_mini_project?nw=nwuserprakhar_raj)**
- **[Finetuned Model Logs](https://wandb.ai/prakhar_raj-iiit-hyderabad/finetune_1?nw=nwuserprakhar_raj)**

---


ЁЯФЧ [Model Checkpoints on onedrive](https://iiithydstudents-my.sharepoint.com/:f:/g/personal/prakhar_raj_students_iiit_ac_in/EgSGY93hPGBCi_pEanDMNkgB44GzrzfsrluRVVlDe--I0A?e=6GD3dJ)

---

## Phase 1 & Phase 2: Data Collection, Preprocessing & Tokenizer Training:

## Corpus Collection
- **Languages and Token Distribution**:
  - English: 50% of total tokens (~1.5 billion tokens).
  - Mother Tongue (Hindi): 30тАУ40% of total tokens (~1тАУ1.2 billion tokens).
  - Indian Language (Awadhi): 10тАУ20% of total tokens (~0.3тАУ0.6 billion tokens).

- **Sources**:
  - English: Extracted from mC4 dataset.
  - Awadhi: Derived from [github_repo](https://github.com/PrashantShuklaa/Awadhi_Speech_Dataset?tab=readme-ov-file) , [Kaggle datasets](https://www.kaggle.com/search?q=awadhi+in%3Adatasets) , [HPLT_dataset](https://hplt-project.org/datasets/v2.0), [huggingface](https://hplt-project.org/datasets/v2.0).
  - Hindi: Combined from Sangraha dataset, mC4, cc100 and other sources.

### Preprocessing
- **Cleaning**:

    - Ensured consistent character representation using NFC.
    - Eliminated invisible characters like zero-width space and BOM.
    - Retained only printable characters and essential whitespace.
    - Deduplicated lines to ensure unique data.
  
- **Sentence Segmentation**:
  - These are used to ensure accurate and language-specific sentence segmentation for both English and Indic languages, which is critical for preparing clean and structured text for language model training.

    -  Split English text into sentences based on punctuation (., ?, !) followed by whitespace or line end.
    - Used Indic NLP Library for Hindi and Awadhi.

  So after separating all sentences:

```text
awadhi_cleaned.txt has 8478186 lines.
english_cleaned.txt has 90641955 lines.
hindi_cleaned.txt has 61054678 lines.
```

---
### For training tokenizer, I have dicided text ratio: (English 50%, Hindi 35%, Awadhi 15%)

- I want tokenizer to see each language proportionally to how we want the LM to learn it.
- And if we just dump most English, it may dominate the vocabulary & Hindi/Awadhi words would appear rarely.

**As seen above if English has ~90 million lines and Awadhi has only approx. ~8 million lines. So just feeding all lines would completely overwhelm Awadhi.**

**so I sampled sentences from each languages to match the target ratio:**

```text
English: 10,000,000 * 0.5  тЙИ 5,000,000 lines
Hindi:   10,000,000 * 0.35 тЙИ 3,500,000 lines
Awadhi:  10,000,000 * 0.15 тЙИ 1,500,000 lines
```

**taking total 10 million lines for training tokenizer**

---

  ## Tokenizer Choice

- **Model**: SentencePiece with unigram model.
- **Vocabulary Size**: 50,000 tokens.

### Training
- Trained the tokenizer on the combined corpus with proportional sampling:
  - English: 50%.
  - Awadhi: 35%.
  - Hindi: 15%.



## Evaluation

  - Verified that the vocabulary supports all three languages adequately.
  - Ensured inclusion of common words and subwords from all three languages.


---

**Token Statistics**:


```text
# your output here
=== English ===
Total chars: 9143092310
Devanagari fraction: 0.000
Total tokens: 2040999488
Tokens per word: 1.317
<unk> tokens: 0 (0.0000)
Byte-fallback tokens: 15004048 (0.0074)

=== Hindi ===
Total chars: 6603725591
Devanagari fraction: 0.774
Total tokens: 1619964264
Tokens per word: 1.245
<unk> tokens: 0 (0.0000)
Byte-fallback tokens: 7492328 (0.0046)

=== Awadhi ===
Total chars: 926348760
Devanagari fraction: 0.772
Total tokens: 226219398
Tokens per word: 1.189
<unk> tokens: 0 (0.0000)
Byte-fallback tokens: 79915 (0.0004)

=== Overall Token Distribution ===
English tokens: 2040999488 (52.51%)
Hindi tokens: 1619964264 (41.67%)
Awadhi tokens: 226219398 (5.82%)
Total tokens: 3887183150
```

### Output for trained tokenizer on dataset:

```text

--- English Tokenization (from dataset) ---
Sentence: This is done by separating isotopes in an enrichment plant to achieve the higher concentration.
Tokens: ['тЦБThis', 'тЦБis', 'тЦБdone', 'тЦБby', 'тЦБseparating', 'тЦБis', 'otope', 's', 'тЦБin', 'тЦБan', 'тЦБenrichment', 'тЦБplant', 'тЦБto', 'тЦБachieve', 'тЦБthe', 'тЦБhigher', 'тЦБconcentration', '.']

Sentence: In reality, the company supplies a full page of testimonials from local users of the app.
Tokens: ['тЦБIn', 'тЦБreality', ',', 'тЦБthe', 'тЦБcompany', 'тЦБsupplies', 'тЦБa', 'тЦБfull', 'тЦБpage', 'тЦБof', 'тЦБtestimonials', 'тЦБfrom', 'тЦБlocal', 'тЦБusers', 'тЦБof', 'тЦБthe', 'тЦБapp', '.']

Sentence: When combined with Work4тАЩs patented technology, FacebookтАЩs targeted advertising platform becomes the must-have tool in every companyтАЩs recruitment tool-box.
Tokens: ['тЦБWhen', 'тЦБcombined', 'тЦБwith', 'тЦБWork', '4', 'тАЩ', 's', 'тЦБpatented', 'тЦБtechnology', ',', 'тЦБFacebook', 'тАЩ', 's', 'тЦБtargeted', 'тЦБadvertising', 'тЦБplatform', 'тЦБbecomes', 'тЦБthe', 'тЦБmust', '-', 'have', 'тЦБtool', 'тЦБin', 'тЦБevery', 'тЦБcompany', 'тАЩ', 's', 'тЦБrecruitment', 'тЦБtool', '-', 'box', '.']



--- Hindi Tokenization (from dataset) ---
Sentence: 14 рдЕрдкреНрд░реИрд▓ 2022 рдХреЛ рджреЛрдиреЛрдВ рдлрд┐рд▓реНрдореЗрдВ рджрд░реНрд╢рдХреЛрдВ рдХреЗ рд╕рд╛рдордиреЗ рдЖрдПрдЧреАред
Tokens: ['тЦБ14', 'тЦБрдЕрдкреНрд░реИрд▓', 'тЦБ2022', 'тЦБрдХреЛ', 'тЦБрджреЛрдиреЛрдВ', 'тЦБрдлрд┐рд▓реНрдореЗрдВ', 'тЦБрджрд░реНрд╢рдХреЛрдВ', 'тЦБрдХреЗ', 'тЦБрд╕рд╛рдордиреЗ', 'тЦБрдЖрдПрдЧреА', 'ред']

Sentence: рд╣реИрд░рдд рдХреА рдмрд╛рдд рдпрд╣ рдХрд┐ рджрдВрдкрддреА рдХреЗ рдмреЗрдЯреЗ рдиреЗ рднреА рдШрдЯрдирд╛ рдХрд╛ рд╡реАрдбрд┐рдпреЛ рдмрдирд╛рдпрд╛ рдерд╛ред
Tokens: ['тЦБрд╣реИ', 'рд░рдд', 'тЦБрдХреА', 'тЦБрдмрд╛рдд', 'тЦБрдпрд╣', 'тЦБрдХрд┐', 'тЦБрджрдВрдкрддреА', 'тЦБрдХреЗ', 'тЦБрдмреЗрдЯреЗ', 'тЦБрдиреЗ', 'тЦБрднреА', 'тЦБрдШрдЯрдирд╛', 'тЦБрдХрд╛', 'тЦБрд╡реАрдбрд┐рдпреЛ', 'тЦБрдмрдирд╛рдпрд╛', 'тЦБрдерд╛', 'ред']

Sentence: рдмреИрдареА рдереА рд╕рд╛рдВрд╕рдж , рд╕рд░ рдХреЗ рдКрдкрд░ рд╕реЗ рдЧрдпреА рдЧреЛрд▓реА !
Tokens: ['тЦБрдмреИрдареА', 'тЦБрдереА', 'тЦБрд╕рд╛рдВрд╕рдж', 'тЦБ', ',', 'тЦБрд╕рд░', 'тЦБрдХреЗ', 'тЦБрдКрдкрд░', 'тЦБрд╕реЗ', 'тЦБрдЧрдпреА', 'тЦБрдЧреЛрд▓реА', 'тЦБ!']


--- Awadhi Tokenization (from dataset) ---
Sentence: рдЗрд╕рдХреЗ рдЕрд▓рд╛рд╡рд╛, рдмреБрджреНрдзрд┐рдорд╛рди рдФрд░рдд рдХреЗ рдЦреЛрдЬ рдХреЗ рд▓рд┐рдП рдЗрд╕ рдпрд╛рддреНрд░рд╛ рдо рдЕрдиреНрдп рдкрд╛рддреНрд░ рд╢рд╛рдорд┐рд▓ рд╣реЛ рд╕рдХрдд рд╣реИрдВ рдФрд░ рд▓рд╛рд░реНрдб рд╕реНрдЯрд╛рд░реНрдХ рдХреЗ рд╕рд╛рде рдЕрдкрдиреЗ рд░рд┐рд╢реНрддреЗ рдкрд░ рдХреМрди рд╕рд╛ рдЕрд╕рд░ рдкрдбрд╝ рд╕рдХрдд рд╣реИ?
Tokens: ['тЦБрдЗрд╕рдХреЗ', 'тЦБрдЕрд▓рд╛рд╡рд╛', ',', 'тЦБрдмреБрджреНрдзрд┐рдорд╛рди', 'тЦБрдФрд░рдд', 'тЦБрдХреЗ', 'тЦБрдЦреЛрдЬ', 'тЦБрдХреЗ', 'тЦБрд▓рд┐рдП', 'тЦБрдЗрд╕', 'тЦБрдпрд╛рддреНрд░рд╛', 'тЦБрдо', 'тЦБрдЕрдиреНрдп', 'тЦБрдкрд╛рддреНрд░', 'тЦБрд╢рд╛рдорд┐рд▓', 'тЦБрд╣реЛ', 'тЦБрд╕рдХрдд', 'тЦБрд╣реИрдВ', 'тЦБрдФрд░', 'тЦБрд▓рд╛рд░реНрдб', 'тЦБрд╕реНрдЯрд╛рд░реНрдХ', 'тЦБрдХреЗ', 'тЦБрд╕рд╛рде', 'тЦБрдЕрдкрдиреЗ', 'тЦБрд░рд┐рд╢реНрддреЗ', 'тЦБрдкрд░', 'тЦБрдХреМрди', 'тЦБрд╕рд╛', 'тЦБрдЕрд╕рд░', 'тЦБрдкрдбрд╝', 'тЦБрд╕рдХрдд', 'тЦБрд╣реИ', '?']

Sentence: рджрдП рдЧрдП рдбреЗрдЯрд╛рд╕реЗрдЯ рдХреЗ рдЖрдзрд╛рд░ рдкрд░, рд╣рдо рдорд╛рди рд╕рдХрдд рд╣реИрдВ рдХрд┐ рдЗрдирдкреБрдЯ рд╕рд┐рдЧреНрдирд▓ рдПрдХ рд╕рд╛рдЗрдирд╕реЛрдЗрдбрд▓ рддрд░рдВрдЧ рд╣реИ рдЬреЗрд╣рд┐рдорд╛ рдПрдХ рд╡рд┐рд╢реЗрд╖ рдЖрд╡реГрддреНрддрд┐ рдЕрдЙрд░ рдЪрд░рдг рд╣реИред
Tokens: ['тЦБрджрдП', 'тЦБрдЧрдП', 'тЦБрдбреЗрдЯрд╛рд╕реЗрдЯ', 'тЦБрдХреЗ', 'тЦБрдЖрдзрд╛рд░', 'тЦБрдкрд░', ',', 'тЦБрд╣рдо', 'тЦБрдорд╛рди', 'тЦБрд╕рдХрдд', 'тЦБрд╣реИрдВ', 'тЦБрдХрд┐', 'тЦБрдЗрдирдкреБрдЯ', 'тЦБрд╕рд┐рдЧреНрдирд▓', 'тЦБрдПрдХ', 'тЦБрд╕рд╛рдЗрди', 'рд╕', 'реЛрдЗрдб', 'рд▓', 'тЦБрддрд░рдВрдЧ', 'тЦБрд╣реИ', 'тЦБрдЬреЗрд╣рд┐рдорд╛', 'тЦБрдПрдХ', 'тЦБрд╡рд┐рд╢реЗрд╖', 'тЦБрдЖрд╡реГрддреНрддрд┐', 'тЦБрдЕрдЙрд░', 'тЦБрдЪрд░рдг', 'тЦБрд╣реИ', 'ред']

Sentence: рд╣рд░ рддрддреНрд╡ рдХреЗ рд▓рд┐рдП, рдЖрдк рдЬрд╛рдВрдЪ рд╕рдХрдд рд╣реИрдВ рдХрд┐ рдИ рдЕрд▓рдордВрдб рд╢рдмреНрджрдХреЛрд╢ рдорд╛ рдПрдХ рдХреБрдВрдЬреА рдХреЗ рд░реВрдк рдорд╛ рдореМрдЬреВрдж рд╣реИред
Tokens: ['тЦБрд╣рд░', 'тЦБрддрддреНрд╡', 'тЦБрдХреЗ', 'тЦБрд▓рд┐рдП', ',', 'тЦБрдЖрдк', 'тЦБрдЬрд╛рдВрдЪ', 'тЦБрд╕рдХрдд', 'тЦБрд╣реИрдВ', 'тЦБрдХрд┐', 'тЦБрдИ', 'тЦБрдЕрд▓', 'рдордВрдб', 'тЦБрд╢рдмреНрджрдХреЛрд╢', 'тЦБрдорд╛', 'тЦБрдПрдХ', 'тЦБрдХреБрдВрдЬреА', 'тЦБрдХреЗ', 'тЦБрд░реВрдк', 'тЦБрдорд╛', 'тЦБрдореМрдЬреВрдж', 'тЦБрд╣реИ', 'ред']



```
---

##  Timeline and Future Steps
###  Work Done So Far
- Completed data collection and preprocessing.
- Trained and evaluated the tokenizer.

###  Next Steps

---

---


### Phase 3: Pretraining the Autoregressive Language Model

This phase focused on training the custom multilingual autoregressive model using the tokenized dataset prepared in the earlier phases. The process involved defining the model architecture, setting up a robust distributed training environment, overcoming significant technical challenges, and successfully training the model for **2** full epochs.

#### Model Architecture

The model is a custom-configured, decoder-only Transformer based on the **Qwen3** architecture. Instead of using a standard pre-trained checkpoint, a new model was initialized from a configuration to match our specific requirements for a smaller, more manageable model.

The key parameters for the model are:
- **Base Architecture**: `Qwen/Qwen3-0.6B` configuration
- **Vocabulary Size**: `50,000` (from our custom tokenizer)
- **Number of Layers**: `12`
- **Number of Attention Heads**: `8`
- **Hidden Size (Embedding Dimension)**: `512`
- **Feed-Forward Intermediate Size**: `2048`

This configuration results in a custom small-scale language model of approximately **120M parameters**, suitable for pretraining with the available resources.

#### Training Configuration

The training was configured using the `transformers.Trainer` and `TrainingArguments` classes. Key hyperparameters were set as follows:

- **Distributed Training Strategy**: Distributed Data Parallel (DDP)
- **Mixed Precision**: `FP16` (Float16) was used to reduce memory consumption and accelerate training.
- **Number of GPUs**: 4
- **Per-Device Batch Size**: `4`
- **Gradient Accumulation Steps**: `8`
- **Effective Batch Size**: `4 (per GPU) * 4 (GPUs) * 8 (accum steps) = 128`
- **Optimizer**: AdamW (default for `Trainer`)
- **Learning Rate**: `1e-3` (0.001)
- **LR Scheduler**: Linear warmup for `500` steps, followed by decay
- **Total Training Epochs**: `2.0`
- **Monitoring**: All metrics were logged to Weights & Biases for real-time tracking: [WandB Link](https://wandb.ai/prakhar_raj-iiit-hyderabad/lma_mini_project/runs/p1rg2omm?nw=nwuserprakhar_raj)


---

### **Pretraining Results**

The model was successfully pretrained for **2 full epochs**. The entire training process took approximately **48 hours** to complete. After training, the final model was evaluated on the held-out test set containing samples from all three languages.

The final evaluation metrics are as follows:

| Metric | Value |
| :--- | :--- |
| **Test Loss** | `3.62` |
| **Perplexity** | `37.33` |
| **Total Epochs** | `2.0` |


<!-- 
![eval_loss](img/eval_loss.svg)
![train_loss](img/train_loss.svg)
![test_perplexity](img/test_perp.svg)
![test_eval_loss](img/test_eval_loss.svg) -->


<p float="left">
  <img src="img/eval_loss.svg" width="45%"/>
  <img src="img/train_loss.svg" width="45%"/>
  <img src="img/test_perp.svg" width="45%"/>
  <img src="img/test_eval_loss.svg" width="45%"/>
</p>


**Conclusion for Phase 3:**
- A test perplexity of **37.33** indicates that the model has successfully learned meaningful patterns, syntax, and vocabulary from the multilingual corpus. Perplexity measures how well the model predicts the next token; a lower value is better.
- For a custom model of this size trained from scratch, this is a strong result and confirms that the pretraining was successful. The final model artifacts have been saved locally and pushed to the Hugging Face Hub at `raja20221020/qwen-small-pretrained`, providing a solid foundation for the fine-tuning tasks in the next phase.



### Phase 4: Fine-tuning for Specific Tasks

With a robust pretrained multilingual model as the foundation, Phase 4 is focused on adapting this model to perform two specialized, instruction-based tasks: **Text Simplification** and **Text De-identification**. This was achieved through a multi-task, multi-lingual fine-tuning process using Parameter-Efficient Fine-Tuning (PEFT) with LoRA.

#### Task 1: FT72 - Text Simplification

The goal of this task is to make complex sentences easier to understand.

**Dataset Curation:**
1.  **Source Data:** The process began by sourcing a high-quality English text simplification dataset (`bogdancazan/wikilarge-text-simplification`). A random sample of 12,000 examples was selected to form the base English dataset.
2.  **Instruction Templating:** To enhance the model's ability to follow instructions, a variety of prompts (e.g., "Simplify this sentence.", "Make this text easier to understand.") were randomly assigned to each example.
3.  **Multilingual Expansion:** The English dataset was then translated into Hindi and Awadhi using the **Google Cloud Translate API**. This step was crucial for creating parallel fine-tuning data for our target languages.

```
{"instruction": "Simplify the following sentence to make it easier to read.", "input": "he was ranked no. in empire magazine s the top movie stars of all time list.", "output": "he is ranked in empire magazine s the top movie stars of all time list."}

{"instruction": "рдирд┐рдореНрдирд▓рд┐рдЦрд┐рдд рдкрд╛рда рдХреЛ рд╕рд░рд▓ рдмрдирд╛рдиреЗ рдХреЗ рд▓рд┐рдП рдкреБрдирдГ рд▓рд┐рдЦреЗрдВред", "input": "рдЙрдирдХреА рдПрдХ рд╕рдВрддрд╛рди рд▓рд┐рди рдЙрд▓рдорд╛рди рд╣реИ, рдЬрд┐рд╕рдХреЗ рдкрд┐рддрд╛ рдЗрдВрдЧрдорд╛рд░ рдмрд░реНрдЧрдореИрди рд╣реИрдВ, рдЬрдмрдХрд┐ рдЙрд▓рдорд╛рди рдХреА рд╢рд╛рджреА рд╕реНрдЯреИрдВрдЧ рд╕реЗ рд╣реБрдИ рдереАред рдЙрд▓рдорд╛рди рдХреЗ рджреЛ рдкреЛрддреЗ-рдкреЛрддрд┐рдпрд╛рдВ рд╣реИрдВ, рдПрдХ рд▓рдбрд╝рдХрд╛ рдФрд░ рдПрдХ рд▓рдбрд╝рдХреА, рдЬреЛ рдЙрдирдХреА рдмреЗрдЯреА рдХреА рджреЛ рд╢рд╛рджрд┐рдпреЛрдВ рд╕реЗ рд╣реИрдВред", "output": "рдорд┐рд╕ рдЙрд▓рдорд╛рди рдХреА рдПрдХ рдмреЗрдЯреА рд▓рд┐рди рдЙрд▓рдорд╛рди рдПрд▓рдЖрд░рдмреА рд╣реИ рдЬреЛ рдЗрдВрдЧрдорд╛рд░ рдмрд░реНрдЧрдореИрди рдФрд░ рджреЛ рдкреЛрддреЗ-рдкреЛрддрд┐рдпреЛрдВ рдХреЗ рд╕рд╛рде рдЖрд░рдЖрд░рдмреА рдореЗрдВ рдкреИрджрд╛ рд╣реБрдИ рдереАред"}

{"instruction": "рдкрд╛рда рдХрд╛ рд╕рд░рд▓ рдмрдирд╛рд╡рд╛ред", "input": "рдордИ рдХрд╛ рдЬрд╛рд░реА рдХреАрди рдЧрд╛ рдИ рдмреНрд▓реЙрдХ рдкрд╛рд░реНрдЯреА рдХреЗ рдкрд╣рд┐рд▓рд╛ рд╡реА рд░рд┐рдХреЙрд░реНрдбреНрд╕ рдПрдк рд░рд╣рд╛ред", "output": "рдИрдкреА рдордИ рдХрд╛ рдкреВрд░реЗ рдпреВрд░реЛрдк рдорд╛ рдЬрд╛рд░реА рдХреАрди рдЧрд╛ рд░рд╣рд╛ред"}

```

4.  **Final Datasets:** The resulting three datasets (English, Hindi, and Awadhi) were individually uploaded to the Hugging Face Hub, creating a comprehensive, multilingual resource for the text simplification task:
### Datasets

| Language | Dataset Link |
|----------|--------------|
| English  | [English Text Simplification](https://huggingface.co/datasets/raja20221020/english-text-simplification-for-finetuning) |
| Hindi    | [Hindi Text Simplification](https://huggingface.co/datasets/raja20221020/hindi-text-simplification-for-finetuning) |
| Awadhi   | [Awadhi Text Simplification](https://huggingface.co/datasets/raja20221020/awadhi-text-simplification-for-finetuning) |



#### Task 2: FT25 - Text De-identification (Anonymization)

The goal of this task is to identify and replace Personally Identifiable Information (PII) with anonymized tags.

**Dataset Curation:**
1.  **Synthetic Data Generation:** Due to the scarcity of public de-identification datasets, a synthetic dataset was generated from scratch.
2.  **Entity Lists:** Comprehensive lists of PII entities (names, addresses, phone numbers, etc.) were compiled for English and Hindi.
3.  **Template Creation:** A large and diverse set of sentence templates containing placeholders for these entities was created for English, Hindi, and Awadhi.
4.  **Data Generation:** A script programmatically filled these templates with random entities from the lists to create realistic "input" sentences. A corresponding "output" sentence was generated by replacing the entities with anonymized tags (e.g., `[NAME]`, `[ADDRESS]`). This process was repeated to generate 12,000 examples for each of 3 languages.

  **Combined Dataset:** The de-identification and text simplification datasets were loaded and processed into a unified instruction format: 
```
{"instruction": "рдЗ рдЯреЗрдХреНрд╕реНрдЯ рдорд╛ рдирд┐рдЬреА рдЬрд╛рдирдХрд╛рд░реА рдХрд╛ рдкрд╣рд┐рдЪрд╛рди рдЫрд┐рдкрд╛рд╡рд╛ред", "input": "рдмрд┐рд▓рд┐рдВрдЧ рдЦрд╛рддрд╛ {ACCOUNT} рдЕрдЙрд░ рдпреЛрдЬрдирд╛ HPN-STU-901 рдХреЗ рдЬрд╛рдВрдЪ рднрдЗрд▓ред", "output": "рдмрд┐рд▓рд┐рдВрдЧ рдЦрд╛рддрд╛ {ACCOUNT} рдЕрдЙрд░ рдпреЛрдЬрдирд╛ [HPBN] рдХреЗ рдЬрд╛рдВрдЪ рднрдЗрд▓ред"}

{"instruction": "Annonymize the following text.", "input": "Credit card 6011111111111117 will expire on 2023-10-27.", "output": "Credit card [CREDIT_CARD] will expire on [DATE]."}

{"instruction": "Annonymize рдХрд░реЗрдВред", "input": "рдЖрдк рдореБрдЭрд╕реЗ 5005566778 рдкрд░ рдпрд╛ рдИрдореЗрд▓ test123@gmail.com рдкрд░ рд╕рдВрдкрд░реНрдХ рдХрд░ рд╕рдХрддреЗ рд╣реИрдВред", "output": "рдЖрдк рдореБрдЭрд╕реЗ [PHONE_NUMBER] рдкрд░ рдпрд╛ рдИрдореЗрд▓ [EMAIL] рдкрд░ рд╕рдВрдкрд░реНрдХ рдХрд░ рд╕рдХрддреЗ рд╣реИрдВред"}
```

5.  **Final Dataset:**  The generated datasets for all three languages were combined, shuffled, and uploaded to the Hugging Face Hub as a single, unified dataset ready for fine-tuning.
### Datasets



| Language | Dataset Link |
|----------|--------------|
| Eng_Hin_Awa | [English Text Simplification](https://huggingface.co/datasets/raja20221020/english_hindi_awadhi_deidentification) |

---

#### Multi-Task Fine-tuning Strategy

To train a single model capable of performing both tasks across all three languages, a multi-task learning approach was implemented.

**Methodology:**
1.  **PEFT with LoRA:** To efficiently adapt the 120M parameter pretrained model without modifying all its weights, **Low-Rank Adaptation (LoRA)** was used. LoRA introduces a small number of trainable parameters into the model's attention layers (`q_proj`, `k_proj`, `v_proj`, etc.), making the fine-tuning process computationally efficient.
2.  **Combined Dataset:** The de-identification and text simplification datasets were loaded and processed into a unified instruction format.
3.  **Interleaved Sampling:** The datasets were combined using `interleave_datasets`, with a sampling probability of **50% for the de-identification task** and **50% for the simplification tasks** (split evenly among the three languages). This ensured the model was trained on a balanced mix of tasks during each training step.
4.  **Validation Strategy:** For datasets lacking a predefined validation split, 10% of the training data was automatically held out to create one, ensuring reliable evaluation of the model's performance on unseen data.

#### Fine-tuning Configuration

- **Base Model**: The final checkpoint from the Phase 3 pretraining.
- **Fine-tuning Method**: LoRA (`r=16`, `lora_alpha=32`)
- **Micro Batch Size**: `8`
- **Gradient Accumulation Steps**: `4`
- **Effective Batch Size**: `32`
- **Learning Rate**: `1e-4`
- **Total Training Epochs**: `2`
- **Monitoring**: All fine-tuning metrics were logged to Weights & Biases.

This comprehensive fine-tuning phase successfully produced a single, versatile, multilingual model adapted for two distinct and practical NLP tasks. The final LoRA adapter, which contains the specialized task knowledge, was saved and pushed to the Hugging Face Hub at `raja20221020/qwen-small-finetuned-multitask`.
  

## Some Examples
```

--- User Prompt ---
рдореЗрд░рд╛ рдлрд╝реЛрди
-------------------

Generating response...

--- Full Model Output ---
рдореЗрд░рд╛ рдлрд╝реЛрди рдпреВрдЬ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП рд╣реИ ред рдореИрдВ рдПрдХ рдмрд╛рд░ рдлреЛрди рдЙрдард╛ рддрд╛ рд╣реВрдВ , рддреЛ рдЖрдкрдХреЛ рдкрддрд╛ рдЪрд▓ рдЬрд╛рддрд╛ рд╣реИ рдХрд┐ рдпрд╣ рдЖрдкрдХреЗ рд▓рд┐рдП рдирд╣реАрдВ рд╣реИ ред ' рд╡рд╣реАрдВ , рдПрдХ рдЕрдиреНрдп рдпреВрдЬрд░ рдиреЗ рд▓рд┐рдЦрд╛ - ' рдЖрдк рдЬрд╛рдирддреЗ рд╣реИрдВ рдХрд┐ рд╣рдо рджреЛрдиреЛрдВ рд╣реА рдмрд╣реБрдд рдЕрдЪреНрдЫреЗ рджреЛрд╕реНрдд рд╣реИрдВ рдФрд░ рдЙрдирдХреА рдорджрдж рдХрд░рдирд╛ рдЪрд╛рд╣рддреЗ рд╣реИрдВ ред ' рдЙрдиреНрд╣реЛрдВрдиреЗ рдЖрдЧреЗ рдХрд╣рд╛ рдХрд┐ ' рдЬрдм рднреА рдореБрдЭреЗ рдХреЛрдИ рд╕рдорд╕реНрдпрд╛ рдЖрддреА рд╣реИ , рддреЛ рдореИрдВ рдЙрдирд╕реЗ рд╕рдВрдкрд░реНрдХ рдХрд░рддрд╛ рд╣реВрдВ ред ' рдмрддрд╛ рджреЗрдВ рдХрд┐ рдЗрд╕рд╕реЗ рдкрд╣рд▓реЗ рдХрдВрдкрдиреА рдиреЗ рдЕрдкрдиреЗ рдпреВрдЬрд░реНрд╕ рдХреЛ рди рдП рд╕реНрдорд╛рд░реНрдЯрдлреЛрди реНрд╕ рдХреА рдкреЗрд╢рдХрд╢ рдХреА рдереА ред рдирдИ рджрд┐рд▓реНрд▓реА ( рдПрдЬреЗрдВрд╕реА / рд╡рд╛рд░реНрддрд╛ ): рджрд┐рд▓реНрд▓реА рдореЗрдВ рдкрд┐рдЫрд▓реЗ рдХ рдИ рджрд┐рдиреЛрдВ рд╕реЗ рд╣реЛ рд░рд╣реА рднрд╛рд░реА рдмрд╛рд░рд┐рд╢ рдХреЗ рдХрд╛рд░рдг рдЖрдЬ рд╕реБрдмрд╣ рд╕реЗ рд╣реА рд▓реЛрдЧреЛрдВ рдХрд╛ рдЬреАрдирд╛ рдореБ рд╣рд╛рд▓ рд╣реЛ рдЧрдпрд╛ рд╣реИ ред рдореМрд╕рдо рд╡рд┐рднрд╛рдЧ рдиреЗ рдЕрдЧрд▓реЗ рджреЛ рджрд┐рдиреЛрдВ рдд рдХ рд╣рд▓реНрдХреА рд╕реЗ рдордзреНрдпрдо рдмрд╛рд░рд┐рд╢ рд╣реЛрдиреЗ рдХреА рд╕рдВрднрд╛рд╡рдирд╛ рдЬрддрд╛ рдп реА рд╣реИ ред 

--- User Prompt ---
My name is John and my email is
-------------------

Generating response...

--- Full Model Output ---
My name is John and my email is ng . I know it ' s your home town , but you have to find a good place for people to meet up with me , s o I can help you out . This was my first time in the US . It was the best thing I ever had ever done before . I love it when I get to know someone who has been there since I was six . I want to be able to talk to them about how they got here and what they did and why they don ' t like it . You are the person who makes me feel more comfortable on the phone as I am not going to let you know that I am here to help you out . I think I will do better than anyone else . Thank you for sharing this .
-------------------------


--- User Prompt ---
рд╣рдо рдкрдВрдбрд┐рддрдЬреА рдХ рдШрд░ рдЬрд╛рдд рд╣рдИрдВ, рдХрд╛рд╣реЗ рдХрд┐ рдЙрд╣рд╛рдБ рдЖрдЬрдХрд╛ рдХрдерд╛ рд╣реЛрдЗ, рдЕрдЙрд░ рдЧрд╛рдБрд╡ рдХреЗ рд╕рдм рд▓реЛрдЧрд╛ рдЙрд╣рд╛рдБ рдЬреБрдЯрд┐рд╣реИрдВред
-------------------

--- Full Model Output ---
рд╣рдо рдкрдВрдбрд┐рдд рдЬреА рдХ рдШрд░ рдЬрд╛рдд рд╣ рдИ рдВ , рдХрд╛рд╣реЗ рдХрд┐ рдЙ рд╣рд╛рдБ рдЖрдЬ рдХрд╛ рдХ рдерд╛ рд╣реЛрдЗ , рдЕрдЙрд░ рдЧрд╛рдБрд╡ рдХреЗ рд╕рдм рд▓реЛрдЧ рд╛ рдЙ рд╣рд╛рдБ рдЬреБрдЯ рд┐ рд╣реИрдВ ред  " рддреБрдо рдХрд╣рд╛рдБ рд╣реЛ ? - рдореИрдВ рддреЛ рдПрдХ рд╕ рди рдХреА рд▓рдбрд╝рдХреА рд╣реВрдБ ! - рд╡рд╣ рдХреНрдпрд╛ рд╣реИ ? - рдпрд╣ рдХреНрдпрд╛ рд╣реИ ? - рд╡реЗ рдХреНрдпрд╛ рд╣реИрдВ ? - рдЕрд░реЗ , рдореИрдВ рддреЛ рдм рд╕ рдПрдХ рд▓рдбрд╝рдХрд╛ рд╣реВрдБ ! - рд▓реЗрдХрд┐рди рдЖрдк рдЬрд╛рдирдд рд╣реИрдВ , рдХреМрди рд╣реИ ? - рдЕрдЧрд░ рдЖрдк рдЕрдкрдиреЗ рдорд╛рддрд╛ - рдкрд┐рддрд╛ рд╕реЗ рдкреНрдпрд╛рд░ рдХрд░рдд рд╣реИрдВ , рддреЛ рдЖрдк рдХреИрд╕реЗ рд╣реИрдВ ? - рдФрд░ рдлрд┐рд░ рднреА рдЖрдк рд╣рдореЗрд╢рд╛ рдореБрдЭрд╕реЗ рдкреНрдпрд╛рд░ рдХрд░рдд рд╣реИрдВ ! - рдЙрдирдХрд╛ рдорддрд▓рдм рдХрд╛ рд╣реИ ? - рдпрд╛ рд░ , рдХрд╛ рдЖрдкрдХрд╛ рд▓рд╛рдЧрдд рд╣реИ рдХрд┐ рдК рдЖрдкрдХреЗ рд╕рд╛рде рд╣реИ ? - рдХрд╛ рдЖрдк рдЪрд╛рд╣рдд рд╣реИрдВ рдХрд┐ рдЙ рдЖрдкрди рджрд┐рд▓ рдорд╛ рдПрдХ рдЦрд╛рд╕ рднрд╛рд╡рдирд╛ рд╡реНрдпрдХреНрдд рдХрд░реИ ? - рдЬрдм рдд рдХ рдЖрдк рдЕрдкрдиреЗ рдЧреБрд░реБ рдХреЗ рдкрд╛рд╕ рдирд╣реАрдВ рдЖрд╡рдд рд╣реИрдВ , рддрдм рдд рдХ рдЖрдк рдЬрдЙрди рдЪреАрдЬ рди рдХрд╛ рд╕рд╛рдЭрд╛ рдХрд░рдд рд╣реИрдВ ,
-------------------------

```

```
(hep_jepa) aryan.kumar@gnode052:~/prakhar$ # --- Test Case 1: English De-identification (Model hallucinates incorrect tags and repeats nonsense) ---
(hep_jepa) aryan.kumar@gnode052:~/prakhar$ python generate.py \
>     --model_type finetuned \
>     --prompt "### Instruction:\n De-identify the text.\n\n### Input:\nMy name is John Smith.\n\n### Response:\n"
--- Starting Inference Script ---
Loading correctly configured tokenizer from: ./ckpt/qwen-small-checkpoints/final
Loading BASE model from: ./ckpt/qwen-small-checkpoints/final
Attaching LoRA adapter from: ./qwen-small-finetuned-adapter/checkpoint-998

--- User Prompt ---
### Instruction:\n De-identify the text.\n\n### Input:\nMy name is John Smith.\n\n### Response:\n
-------------------

Generating response...

--- Full Model Output ---
# # # Instruction : \ n De - identify the text . \ n \ n # # # Input : \ n My name is John Smith . \ n \ n # # # Response : \ n ЁЯП╗ \ n [ MY FILE NAME ] has been recorded . \ n \ n [ FF FF _ ID ] has been recorded . \ n \ r @ y a h o o . org was recorded . \ n \ r [ MY FILE NAME ]. \ n \ r [ MY FILE NAME ]. \ n \ r [ MY FILE NAME ]. \ n \ r [ FF FF _ ID ] had been recorded . \ n \ r [ MY FILE NAME ]. \ n \ r [ MY FILE NAME .тАЭ was recorded . \ n \ r [ MY FILE NAME ]. \ n \ r [ FF FF _ ID ] had been recorded . \ n \ r [ MY FILE NAME ]. \ n \ r [ FF FF _ ID ]. was recorded . \ n
-------------------------


(hep_jepa) aryan.kumar@gnode052:~/prakhar$ # --- Test Case 2: English De-identification (Model mangles the prompt and outputs repetitive gibberish) ---
(hep_jepa) aryan.kumar@gnode052:~/prakhar$ python generate.py \
>     --model_type finetuned \
>     --prompt "### Instruction:\n De-identify the PII text.\n\n### Input:\n my number is 555-1234.\n\n### Response:\n"
--- Starting Inference Script ---
Loading correctly configured tokenizer from: ./ckpt/qwen-small-checkpoints/final
Loading BASE model from: ./ckpt/qwen-small-checkpoints/final
Attaching LoRA adapter from: ./qwen-small-finetuned-adapter/checkpoint-100

--- User Prompt ---
### Instruction:\n De-identify the PII text.\n\n### Input:\n my number is 555-1234.\n\n### Response:\n
-------------------

Generating response...

--- Full Model Output ---
# # # Instruction : \ n De - identify the PI I text . \ n \ n # # # Input : \ n my number is 5 55 - 1234 . \ n \ n # # # Response : \ n рд╛рд░реНрдЯрдирд░рд╢рд┐рдк _ ID [ _ ID ] was _ ID [ \ n ] ; \ n \ n _ ID [ _ ID ] was _ ID [ \ n ]. \ n \ n # # # Response : \ n my number is 4 40 - 10 42 . \ n \ n # # Response : \ n my number is 2 98 - 09 3 . \ n \ n # # Response : \ n my number is 60 31 - 08 4 . \ n \ n # # Response : \ n my number is 1 888 - 13 57 . \ n \ n # # Response : \ n my number is 70 33 - 18 58 . \ n \ n # # Response : \ n my number is 80 28 - 17 77 . \ n \

```
